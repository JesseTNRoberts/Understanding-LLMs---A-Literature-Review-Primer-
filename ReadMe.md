## A List of Papers to Help with Understanding LLMs

### Foundations of the Transformer Architecture

- T. Mikolov, K. Chen, G. Corrado, and J. Dean, “Efficient Estimation of Word Representations in Vector Space,” Sep. 06, 2013, arXiv: arXiv:1301.3781. Accessed: Aug. 20, 2024. [Online]. Available: http://arxiv.org/abs/1301.3781

- D. Bahdanau, K. Cho, and Y. Bengio, “NEURAL MACHINE TRANSLATION BY JOINTLY LEARNING TO ALIGN AND TRANSLATE,” 2015.

- A. Vaswani et al., “Attention is All you Need,” 2017.


### Foundational LLMs that Everyone Should Know

- J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, “BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,” 2018.

- A. Radford, K. Narasimhan, T. Salimans, and I. Sutskever, “Improving Language Understanding by Generative Pre-Training,” 2018.

- M. Lewis et al., “BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension,” 2020.


### Important Emergent Capabilities

- A. Radford, J. Wu, R. Child, D. Luan, D. Amodei, and I. Sutskever, “Language Models are Unsupervised Multitask Learners,” 2019.

- T. B. Brown et al., “Language Models are Few-Shot Learners,” Jul. 22, 2020, arXiv: arXiv:2005.14165. Accessed: Aug. 21, 2024. [Online]. Available: 
http://arxiv.org/abs/2005.14165

- J. Wei et al., “Chain-of-Thought Prompting Elicits Reasoning in Large Language Models,” 2022.


### Improving the Transformer Architecture and Training for Better LLMs

- I. Beltagy, M. E. Peters, and A. Cohan, “Longformer: The Long-Document Transformer,” 2020.

- P. Lewis et al., “Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks,” 2020.

- J. Su, “RoFormer: Enhanced transformer with Rotary Position Embedding,” 2021.

- L. Ouyang et al., “Training language models to follow instructions with human feedback,” 2022.

- T. Dettmers, A. Pagnoni, A. Holtzman, and L. Zettlemoyer, “QLORA: Efficient Finetuning of Quantized LLMs,” 2023.


### Deeper Empirical Understanding

- A. Rogers, O. Kovaleva, and A. Rumshisky, “A Primer in BERTology: What We Know About How BERT Works,” 2020.

- Z. Liu, O. Kitouni, N. Nolte, E. J. Michaud, M. Tegmark, and M. Williams, “Towards Understanding Grokking: An Effective Theory of Representation Learning,” 2022.


### Important Models

- A. Dubey et al., “The Llama 3 Herd of Models,” Aug. 15, 2024, arXiv: arXiv:2407.21783. Accessed: Aug. 20, 2024. [Online]. Available: http://arxiv.org/abs/2407.21783

- A. Q. Jiang et al., “Mixtral of Experts,” Jan. 08, 2024, arXiv: arXiv:2401.04088. Accessed: Aug. 20, 2024. [Online]. Available: http://arxiv.org/abs/2401.04088


### What's next?

- A. Gu and T. Dao, “Mamba: Linear-Time Sequence Modeling with Selective State Spaces,” 2023.

- Zhang, J., Nolte, N., Sadhukhan, R., Chen, B., & Bottou, L. (2024). Memory Mosaics. arXiv preprint arXiv:2405.06394.
